{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb9486e",
   "metadata": {},
   "source": [
    "We will now define the dataset class for our image dataset consisting of keypoint annotations inside images. When computing heatmaps(one for each keypoint present in an image), we will also make heatmaps for lines present in the annotation jsons for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d056e",
   "metadata": {
    "time_run": "2026-02-15T19:31:02.048536+00:00"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from typing import List, Tuple, Callable, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import hydra\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cf3d3",
   "metadata": {
    "time_run": "2026-02-15T19:31:02.053624+00:00"
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf, DictConfig\n",
    "cfg = OmegaConf.load('train_config.yaml')\n",
    "hrnet_cfg = OmegaConf.load('model_config/hrnet_w48.yaml')\n",
    "cfg.model.params.nn_module.hrnet_config = hrnet_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11de15",
   "metadata": {
    "time_run": "2026-02-15T19:31:02.499111+00:00"
   },
   "outputs": [],
   "source": [
    "if str(Path.cwd()) not in sys.path: sys.path.insert(0, str(Path.cwd()))\n",
    "import metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1133df",
   "metadata": {
    "time_run": "2026-02-15T19:31:05.819094+00:00"
   },
   "outputs": [],
   "source": [
    "class HRNetDataset(Dataset):\n",
    "    def __init__(self, dataset_folder: str, transform: Optional[Callable] = None, num_keypoints: int = 30, img_size: Tuple[int, int] = (960, 540), margin: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.dataset_folder,self.num_keypoints,self.transform,self.img_size,self.margin = dataset_folder,num_keypoints,transform,img_size,margin\n",
    "        self.img_paths = [p for p in Path(dataset_folder).glob('*.jpg') if p.with_suffix('.json').exists()][:1]        \n",
    "    \n",
    "    def __len__(self): return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "        if self.transform: sample = self.transform(sample)\n",
    "        annot_path = img_path.with_suffix('.json')\n",
    "        keypoints,mask,lines = self._annot2keypoints(annot_path)\n",
    "        image,keypoints = self._resize_img_and_kpts(image, keypoints)\n",
    "        sample = dict(image=image, keypoints=keypoints, img_idx=idx, mask=mask, img_name=img_path.name, lines=lines)\n",
    "        return sample\n",
    "    \n",
    "    def _annot2keypoints(self, annot_path):\n",
    "        with open(annot_path) as f: data = json.loads(f.read())\n",
    "        kpts_dict,lines = {},[]\n",
    "        for shape in data['shapes']:\n",
    "            if shape['shape_type'] == 'point': kpts_dict[int(shape['label'])] = shape['points'][0]\n",
    "            elif shape['shape_type'] == 'linestrip': lines.append(dict(label=shape['label'], points=shape['points']))\n",
    "        keypoints = np.ones(self.num_keypoints * 3, dtype=np.float32) * -1\n",
    "        mask = np.ones(self.num_keypoints, dtype=int)\n",
    "        for i in range(self.num_keypoints):\n",
    "            if i in kpts_dict:\n",
    "                keypoints[i*3:i*3+2] = kpts_dict[i]\n",
    "                keypoints[i*3+2] = 1\n",
    "                mask[i] = 0\n",
    "            else: keypoints[i*3+2] = 0\n",
    "        return keypoints,mask,lines\n",
    "    \n",
    "    def _resize_img_and_kpts(self, image, keypoints):\n",
    "        h,w = image.shape[:2]\n",
    "        tw,th = self.img_size\n",
    "        scale_h,scale_w = th/h,tw/w\n",
    "        resized_img = cv2.resize(image, (tw,th))\n",
    "        for i in range(self.num_keypoints):\n",
    "            if keypoints[i*3+2] > 0: keypoints[i*3],keypoints[i*3+1] = keypoints[i*3]*scale_w,keypoints[i*3+1]*scale_h\n",
    "        return resized_img,keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117d8ba",
   "metadata": {
    "time_run": "2026-02-15T19:31:05.825231+00:00"
   },
   "outputs": [],
   "source": [
    "def gaussian(x: torch.Tensor, mu: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "    \"\"\"1D Gaussian distribution. The distribution amplitude is 1.0.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): 1D tensor of X values, (X,).\n",
    "        mu (torch.Tensor): Mean values for gaussian (B, N).\n",
    "        sigma (float): Standard deviation in scale of X axis.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Resulted 1d gaussians: (B, N, X).\n",
    "    \"\"\"\n",
    "    return torch.exp(-(torch.div(x - mu.unsqueeze(-1), sigma) ** 2) / 2.0)\n",
    "\n",
    "def create_heatmaps(keypoints: torch.Tensor, sigma: float, pred_size: Tuple[int, int] = (540, 960)) -> torch.Tensor:\n",
    "    h,w = pred_size\n",
    "    device = keypoints.device\n",
    "    x,y,vis = keypoints[:,:,0],keypoints[:,:,1],keypoints[:,:,2]\n",
    "    x_range,y_range = torch.arange(0, w, device=device, dtype=torch.float32),torch.arange(0, h, device=device, dtype=torch.float32)\n",
    "    gauss_x,gauss_y = gaussian(x_range, x, sigma),gaussian(y_range, y, sigma)\n",
    "    heatmaps = torch.einsum(\"BNW, BNH -> BNHW\", gauss_x, gauss_y)\n",
    "    heatmaps = torch.where((vis > 0).unsqueeze(-1).unsqueeze(-1), heatmaps, torch.tensor(0.0, device=device))\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45a50f",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Why do you need to unsqueeze vis two times? walk me throuugh the shapes step by step\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Let's trace through the shapes step by step:\n",
    "\n",
    "Starting shapes:\n",
    "- `keypoints`: (B, N, 3) where B=batch size, N=num_keypoints\n",
    "- `x, y, vis`: each is (B, N) after slicing\n",
    "- `x_range`: (W,) and `y_range`: (H,)\n",
    "\n",
    "After gaussian function:\n",
    "- `gauss_x`: (B, N, W) - Gaussian values for each keypoint along width\n",
    "- `gauss_y`: (B, N, H) - Gaussian values for each keypoint along height\n",
    "\n",
    "After einsum:\n",
    "- `heatmaps`: (B, N, H, W) - 2D Gaussian heatmap for each keypoint\n",
    "\n",
    "Now for the masking with `vis`:\n",
    "- `vis`: (B, N) - visibility flag for each keypoint\n",
    "- Need to broadcast `vis` to match `heatmaps` shape (B, N, H, W)\n",
    "- First `unsqueeze(-1)`: (B, N, 1) - adds dimension for H\n",
    "- Second `unsqueeze(-1)`: (B, N, 1, 1) - adds dimension for W\n",
    "- Now `(vis > 0).unsqueeze(-1).unsqueeze(-1)` broadcasts correctly to (B, N, H, W)\n",
    "\n",
    "The two unsqueezes are needed because `heatmaps` has 4 dimensions and `vis` only has 2, so we need to add 2 singleton dimensions to enable broadcasting across both the H and W dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84ddc8",
   "metadata": {
    "time_run": "2026-02-15T19:31:06.699139+00:00"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset, default_collate\n",
    "from typing import List, Optional, Callable\n",
    "from transforms import *\n",
    "\n",
    "collate_objs = ['keypoints', 'image', 'img_idx', 'mask']\n",
    "\n",
    "def custom_collate(batch):\n",
    "    default_collated = default_collate([{k: v for k, v in sample.items()\n",
    "                                         if k in collate_objs}\n",
    "                                        for sample in batch])\n",
    "    custom_collated = {'img_name': [sample['img_name'] for sample in batch]}\n",
    "\n",
    "    return {**default_collated, **custom_collated}\n",
    "\n",
    "def train_transform(brightness: Tuple[float, float] = (0.8, 1.2),\n",
    "                    color: Tuple[float, float] = (0.8, 1.2),\n",
    "                    contrast: Tuple[float, float] = (0.8, 1.2),\n",
    "                    gauss_noise_sigma: float = 30.0,\n",
    "                    prob: float = 0.5):\n",
    "    transforms = ComposeTransform([\n",
    "        UseWithProb(ColorAugment(brightness=brightness,\n",
    "                                 color=color,\n",
    "                                 contrast=contrast), prob),\n",
    "        UseWithProb(GaussNoise(gauss_noise_sigma), prob),\n",
    "        # UseWithProb(Flip(), 0.5),\n",
    "        ToTensor()\n",
    "    ])\n",
    "    return transforms\n",
    "    \n",
    "def get_loader(dataset_paths: List[str], data_params: DictConfig,\n",
    "\n",
    "               transform: Optional[Callable] = None, shuffle: bool = True)\\\n",
    "        -> DataLoader:\n",
    "    datasets = []\n",
    "    for dataset_path in dataset_paths:\n",
    "        datasets.append(HRNetDataset(dataset_path, transform=transform,\n",
    "                                     num_keypoints=data_params.num_keypoints,\n",
    "                                     margin=data_params.margin))\n",
    "    dataset = ConcatDataset(datasets)\n",
    "    factor = 1 if shuffle else 2\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=data_params.batch_size * factor,\n",
    "        num_workers=data_params.num_workers,\n",
    "        pin_memory=data_params.pin_memory,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=custom_collate)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ac575",
   "metadata": {
    "time_run": "2026-02-15T19:31:13.552404+00:00"
   },
   "outputs": [],
   "source": [
    "def plot_heatmap_on_img(img_tensor, heatmap_tensor):\n",
    "    img = img_tensor.detach().cpu().numpy()\n",
    "    heatmap = heatmap_tensor.detach().cpu().numpy()\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap_colored = cv2.applyColorMap((heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "    overlay = cv2.addWeighted(img.astype(np.uint8), 0.6, heatmap_colored, 0.4, 0)\n",
    "    plt.imshow(overlay)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06ef96",
   "metadata": {
    "time_run": "2026-02-11T21:37:05.557124+00:00"
   },
   "outputs": [],
   "source": [
    "train_loader = get_loader(cfg.data.train, cfg.data_params, None, True)\n",
    "dl = iter(train_loader)\n",
    "\n",
    "for batch in dl:\n",
    "    for idx in range(cfg.data_params.batch_size):\n",
    "        img, keypoints, mask = batch['image'][idx], batch['keypoints'][idx].reshape(-1, cfg.data_params.num_keypoints, 3), batch['mask'][idx]\n",
    "        heatmaps = create_heatmaps(keypoints, 2)\n",
    "        heatmaps = torch.cat(\n",
    "                [heatmaps, (1.0 - torch.max(heatmaps, dim=1, keepdim=True)[0])], 1)\n",
    "        maps = torch.sum(heatmaps[0][:-1], 0)\n",
    "        plot_heatmap_on_img(img, maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2968b5",
   "metadata": {
    "time_run": "2026-02-11T21:37:11.875412+00:00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/data/metamodel.py:23: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler() if self.amp else None\n",
      "/usr/local/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "model = hydra.utils.instantiate(cfg.model)\n",
    "aug_params = cfg.data_params.augmentations\n",
    "train_trns = train_transform(\n",
    "    brightness=aug_params.brightness,\n",
    "    color=aug_params.color,\n",
    "    contrast=aug_params.contrast,\n",
    "    gauss_noise_sigma=aug_params.gauss_noise_sigma,\n",
    "    prob=aug_params.prob\n",
    ")\n",
    "val_trns = test_transform()\n",
    "train_loader = get_loader(cfg.data.train, cfg.data_params,\n",
    "                            train_trns, True)\n",
    "# val_loader = get_loader(cfg.data.val, cfg.data_params, val_trns, False)\n",
    "# experiment_name = cfg.metadata.experiment_name\n",
    "# run_name = cfg.metadata.run_name\n",
    "# save_dir = f'./experiments/{experiment_name}_{run_name}'\n",
    "# callbacks = [\n",
    "#     Checkpoint(save_dir, max_saves=3, file_format='save-{epoch:03d}.pth',\n",
    "#                 save_after_exception=True, optimizer_state=True, period=2),\n",
    "#     LoggingToFile(os.path.join(save_dir, 'log.txt')),\n",
    "# ]\n",
    "\n",
    "# pretrain_path = cfg.model.params.pretrain\n",
    "# if pretrain_path is not None:\n",
    "#     if os.path.exists(pretrain_path):\n",
    "#         model_pretrain = load_model(pretrain_path,\n",
    "#                                     device=cfg.model.params.device)\n",
    "#         if cfg.train_params.load_compatible:\n",
    "#             model_pretrain = load_model(pretrain_path,\n",
    "#                                         device=cfg.model.params.device)\n",
    "#             model = load_compatible_weights(model_pretrain, model)\n",
    "#         else:\n",
    "#             model = load_model(pretrain_path,\n",
    "#                                 device=cfg.model.params.device)\n",
    "#         model.set_lr(cfg.model.params.optimizer.lr)\n",
    "#     else:\n",
    "#         raise ValueError(f'Pretrain {pretrain_path} does not exist')\n",
    "# # Model may need tuning to find the optimal one for the particular model\n",
    "# if cfg.train_params.use_compile:\n",
    "#     model.nn_module = compile(model.nn_module)\n",
    "# model.fit(train_loader, val_loader=val_loader, metrics_on_train=False,\n",
    "#             num_epochs=cfg.train_params.max_epochs,\n",
    "#             callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "concise",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
