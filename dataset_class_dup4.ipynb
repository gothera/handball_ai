{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb9486e",
   "metadata": {
    "time_run": "2026-02-03T19:49:47.416410+00:00"
   },
   "source": [
    "We will now define the dataset class for our image dataset consisting of keypoint annotations inside images. When computing heatmaps(one for each keypoint present in an image), we will also make heatmaps for lines present in the annotation jsons for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d056e",
   "metadata": {
    "time_run": "2026-02-05T19:15:33.345413+00:00"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from typing import List, Tuple, Callable, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cf3d3",
   "metadata": {
    "time_run": "2026-02-05T19:31:17.816037+00:00"
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from omegaconf import DictConfig\n",
    "cfg = OmegaConf.load('train_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1133df",
   "metadata": {
    "time_run": "2026-02-05T19:31:19.954893+00:00"
   },
   "outputs": [],
   "source": [
    "class HRNetDataset(Dataset):\n",
    "    def __init__(self, dataset_folder: str, transform: Optional[Callable] = None, num_keypoints: int = 30, img_size: Tuple[int, int] = (960, 540), margin: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.dataset_folder,self.num_keypoints,self.transform,self.img_size,self.margin = dataset_folder,num_keypoints,transform,img_size,margin\n",
    "        self.img_paths = [p for p in Path(dataset_folder).glob('*.jpg') if p.with_suffix('.json').exists()]        \n",
    "    \n",
    "    def __len__(self): return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "        sample = dict(image=image)\n",
    "        if self.transform: sample = self.transform(sample)\n",
    "        annot_path = img_path.with_suffix('.json')\n",
    "        keypoints,mask,lines = self._annot2keypoints(annot_path)\n",
    "        sample.update(dict(keypoints=keypoints, img_idx=idx, mask=mask, img_name=img_path.name, lines=lines))\n",
    "        return sample\n",
    "    \n",
    "    def _annot2keypoints(self, annot_path):\n",
    "        with open(annot_path) as f: data = json.loads(f.read())\n",
    "        kpts_dict,lines = {},[]\n",
    "        for shape in data['shapes']:\n",
    "            if shape['shape_type'] == 'point': kpts_dict[int(shape['label'])] = shape['points'][0]\n",
    "            elif shape['shape_type'] == 'linestrip': lines.append(dict(label=shape['label'], points=shape['points']))\n",
    "        keypoints = np.ones(self.num_keypoints * 3, dtype=np.float32) * -1\n",
    "        mask = np.ones(self.num_keypoints, dtype=int)\n",
    "        for i in range(self.num_keypoints):\n",
    "            if i in kpts_dict:\n",
    "                keypoints[i*3:i*3+2] = kpts_dict[i]\n",
    "                keypoints[i*3+2] = 1\n",
    "                mask[i] = 0\n",
    "            else: keypoints[i*3+2] = 0\n",
    "        return keypoints,mask,lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117d8ba",
   "metadata": {
    "time_run": "2026-02-05T19:31:20.540006+00:00"
   },
   "outputs": [],
   "source": [
    "def create_heatmaps(keypoints: torch.Tensor, sigma: float,\n",
    "                    pred_size: Tuple[int, int] = (540, 960)) -> torch.Tensor:\n",
    "    \"\"\"Create Gaussian distributions heatmaps for keypoints.\n",
    "\n",
    "    Each heatmap is drawn on an individual channel.\n",
    "\n",
    "    Args:\n",
    "        keypoints (torch.Tensor): A batch (B) of N points, each point is (x, y).\n",
    "            Expected shape: (B, N, 2).\n",
    "        sigma (float): Standard deviation.\n",
    "        pred_size (Tuple[int, int]): Size of the 2D Gaussian distribution canvas\n",
    "            (H, W). Defaults to (68, 120).\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Resulted Gaussian heatmaps: (B, N, H, W).\n",
    "\n",
    "    \"\"\"\n",
    "    h, w = pred_size\n",
    "    device = keypoints.device\n",
    "    x = keypoints[:, :, 0]\n",
    "    y = keypoints[:, :, 1]\n",
    "\n",
    "    x_range = torch.arange(0, w, device=device, dtype=torch.float32)\n",
    "    y_range = torch.arange(0, h, device=device, dtype=torch.float32)\n",
    "    gauss_x: torch.Tensor = gaussian(x_range, x, sigma)\n",
    "    gauss_y: torch.Tensor = gaussian(y_range, y, sigma)\n",
    "    heatmaps = torch.einsum(\"BNW, BNH -> BNHW\", gauss_x, gauss_y)\n",
    "\n",
    "    visible_points = torch.any(keypoints == 1, dim=-1, keepdim=True)\n",
    "    zero = torch.tensor(0.0, device=device, dtype=torch.float32)\n",
    "    heatmaps = torch.where(visible_points.unsqueeze(-1), heatmaps, zero)\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84ddc8",
   "metadata": {
    "time_run": "2026-02-05T19:31:44.539409+00:00"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'custom_collate' from 'torch.utils.data' (/usr/local/lib/python3.12/site-packages/torch/utils/data/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, ConcatDataset, custom_collate",
      "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_loader\u001b[39m(dataset_paths: List[\u001b[38;5;28mstr\u001b[39m], data_params: DictConfig,",
      "\u001b[32m      4\u001b[39m ",
      "\u001b[32m      5\u001b[39m                transform: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m, shuffle: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m)\\",
      "\u001b[32m      6\u001b[39m         -> DataLoader:",
      "\u001b[32m      7\u001b[39m     datasets = []",
      "",
      "\u001b[31mImportError\u001b[39m: cannot import name 'custom_collate' from 'torch.utils.data' (/usr/local/lib/python3.12/site-packages/torch/utils/data/__init__.py)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset, custom_collate\n",
    "\n",
    "def get_loader(dataset_paths: List[str], data_params: DictConfig,\n",
    "\n",
    "               transform: Optional[Callable] = None, shuffle: bool = True)\\\n",
    "        -> DataLoader:\n",
    "    datasets = []\n",
    "    for dataset_path in dataset_paths:\n",
    "        datasets.append(HRNetDataset(dataset_path, transform=transform,\n",
    "                                     num_keypoints=data_params.num_keypoints,\n",
    "                                     margin=data_params.margin))\n",
    "    dataset = ConcatDataset(datasets)\n",
    "    factor = 1 if shuffle else 2\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=data_params.batch_size * factor,\n",
    "        num_workers=data_params.num_workers,\n",
    "        pin_memory=data_params.pin_memory,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=custom_collate)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06ef96",
   "metadata": {
    "time_run": "2026-02-05T19:31:22.398427+00:00"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_collate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m",
      "\u001b[32m      1\u001b[39m dataset = HRNetDataset(dataset_folder=\u001b[33m'\u001b[39m\u001b[33m./clean_keypoints_dataset/\u001b[39m\u001b[33m'\u001b[39m)",
      "\u001b[32m      2\u001b[39m sample = dataset[\u001b[32m0\u001b[39m]",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_loader = \u001b[43mget_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m",
      "\u001b[32m      4\u001b[39m dl = \u001b[38;5;28miter\u001b[39m(train_loader)",
      "\u001b[32m      6\u001b[39m batch = \u001b[38;5;28mnext\u001b[39m(dl)",
      "",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mget_loader\u001b[39m\u001b[34m(dataset_paths, data_params, transform, shuffle)\u001b[39m",
      "\u001b[32m     12\u001b[39m dataset = ConcatDataset(datasets)",
      "\u001b[32m     13\u001b[39m factor = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m2\u001b[39m",
      "\u001b[32m     14\u001b[39m loader = DataLoader(",
      "\u001b[32m     15\u001b[39m     dataset, batch_size=data_params.batch_size * factor,",
      "\u001b[32m     16\u001b[39m     num_workers=data_params.num_workers,",
      "\u001b[32m     17\u001b[39m     pin_memory=data_params.pin_memory,",
      "\u001b[32m     18\u001b[39m     shuffle=shuffle,",
      "\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     collate_fn=\u001b[43mcustom_collate\u001b[49m)",
      "\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loader",
      "",
      "\u001b[31mNameError\u001b[39m: name 'custom_collate' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = HRNetDataset(dataset_folder='./clean_keypoints_dataset/')\n",
    "sample = dataset[0]\n",
    "train_loader = get_loader(cfg.data.train, cfg.data_params, None, True)\n",
    "dl = iter(train_loader)\n",
    "\n",
    "batch = next(dl)\n",
    "img, keypoints, mask = batch['image'][0], batch['keypoints'][0].reshape(-1, cfg.data_params.num_keypoints, 3), batch['mask'][0]\n",
    "print(img.shape, keypoints.shape, mask.shape)\n",
    "\n",
    "for batch in dl:\n",
    "    for idx in range(cfg.data_params.batch_size):\n",
    "        img, keypoints, mask = batch['image'][idx], batch['keypoints'][idx].reshape(-1, cfg.data_params.num_keypoints, 3), batch['mask'][idx]\n",
    "        # print(img.shape, keypoints.shape, mask.shape)\n",
    "        heatmaps = create_heatmaps(keypoints, 1.0)\n",
    "        heatmaps = torch.cat(\n",
    "                [heatmaps, (1.0 - torch.max(heatmaps, dim=1, keepdim=True)[0])], 1)\n",
    "        maps = torch.sum(heatmaps[0][:-1], 0)\n",
    "        # plot_heatmap(maps)\n",
    "\n",
    "        plot_img_keypoints(img.detach().cpu().numpy(), maps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860284a",
   "metadata": {
    "solveit_ai": true,
    "time_run": "2026-02-05T19:10:35.060923+00:00"
   },
   "source": [
    "6 self.img_paths = filter(sorted([p for p in Path(dataset_folder).glob('*.jpg')]), lambda x: os.path.exists(x.replace('.jpg', '.json')))\n",
    "\n",
    "TypeError: 'function' object is not iterable\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "The issue is that `filter` returns a filter object, not a list, and the arguments are in the wrong order. Here's the fix:\n",
    "\n",
    "```python\n",
    "class HRNetDataset(Dataset):\n",
    "    def __init__(self, dataset_folder: str, transform: Optional[Callable] = None, num_keypoints: int = 30, img_size: Tuple[int, int] = (960, 540), margin: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.dataset_folder,self.num_keypoints,self.transform,self.img_size,self.margin = dataset_folder,num_keypoints,transform,img_size,margin\n",
    "        all_imgs = sorted([p for p in Path(dataset_folder).glob('*.jpg')])\n",
    "        self.img_paths = [p for p in all_imgs if p.with_suffix('.json').exists()]\n",
    "        \n",
    "    def __len__(self): return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "        sample = dict(image=image)\n",
    "        if self.transform: sample = self.transform(sample)\n",
    "        annot_path = img_path.with_suffix('.json')\n",
    "        keypoints,mask,lines = self._annot2keypoints(annot_path)\n",
    "        sample.update(dict(keypoints=keypoints, img_idx=idx, mask=mask, img_name=img_path.name, lines=lines))\n",
    "        return sample\n",
    "    \n",
    "    def _annot2keypoints(self, annot_path):\n",
    "        with open(annot_path) as f: data = json.loads(f.read())\n",
    "        kpts_dict,lines = {},[]\n",
    "        for shape in data['shapes']:\n",
    "            if shape['shape_type'] == 'point': kpts_dict[int(shape['label'])] = shape['points'][0]\n",
    "            elif shape['shape_type'] == 'linestrip': lines.append(dict(label=shape['label'], points=shape['points']))\n",
    "        keypoints = np.ones(self.num_keypoints * 3, dtype=np.float32) * -1\n",
    "        mask = np.ones(self.num_keypoints, dtype=int)\n",
    "        for i in range(self.num_keypoints):\n",
    "            if i in kpts_dict:\n",
    "                keypoints[i*3:i*3+2] = kpts_dict[i]\n",
    "                keypoints[i*3+2] = 1\n",
    "                mask[i] = 0\n",
    "            else: keypoints[i*3+2] = 0\n",
    "        return keypoints,mask,lines\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "concise",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
